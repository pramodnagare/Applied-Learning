{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCP Big Data Analytics Pipeline\n",
    "\n",
    "We are going to develope GCP data analytics pipelie\n",
    "with Batch Processing\n",
    "and Real-Time data streaming\n",
    "\n",
    "<img src=\"images/pipeline.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-cloud\n",
    "# !pip install google-cloud-pubsub\n",
    "# !pip install google.cloud_bigquery\n",
    "# !pip install google.cloud_storage\n",
    "\n",
    "from google.oauth2 import service_account #For GCP Account connection\n",
    "from google.cloud import pubsub_v1 # For PubSub Client\n",
    "from google.cloud import bigquery # For BigQuery Client\n",
    "from google.cloud import storage # For Cloud Storage Client\n",
    "\n",
    "import json # For Message syntax\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import concurrent.futures\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_configuration(file):\n",
    "    try:\n",
    "        conf = open(file).read()\n",
    "        conf = json.loads(conf)\n",
    "        values = list(conf.values())\n",
    "        if None in values:\n",
    "            print(\"Please confirm all fields are mentioned in the credential files! Try again!\")\n",
    "        return conf\n",
    "    except:\n",
    "        print(\"Errro Occurred! Please check if file is available!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = load_configuration(\"./Configuration/conf.json\")\n",
    "cred = service_account.Credentials.from_service_account_file(conf[\"cred_file\"])\n",
    "project_id = conf[\"project_id\"]\n",
    "project_name = conf[\"project_name\"]\n",
    "topic_name = conf[\"topic_name\"]\n",
    "bucket_name = conf[\"bucket_name\"]\n",
    "subscription_name = conf[\"subscription_name\"]\n",
    "dataset_name = conf[\"dataset_name\"]\n",
    "table_name = conf[\"table_name\"]\n",
    "schema = conf[\"schema\"]\n",
    "job_name = conf[\"job_name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Pipeline components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storage Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket aws-review-data created.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Creating a Cloud Storag Bucket:\n",
    "storage_client = storage.Client(project=project_id,credentials=cred)\n",
    "bucket = storage_client.create_bucket(bucket_name)\n",
    "print('Bucket {} created.'.format(bucket.name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/storage.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PubSub Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic created: name: \"projects/gcp-bigdata-analytics-pipeline/topics/gcp-topic\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Creating PubSub Topic:\n",
    "publisher = pubsub_v1.PublisherClient(credentials=cred)\n",
    "topic_path = publisher.topic_path(project_id, topic_name)\n",
    "topic = publisher.create_topic(topic_path)\n",
    "print('Topic created: {}'.format(topic))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/topic.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PubSub Subscription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subscription created: name: \"projects/gcp-bigdata-analytics-pipeline/subscriptions/gcp-subscription\"\n",
      "topic: \"projects/gcp-bigdata-analytics-pipeline/topics/gcp-topic\"\n",
      "push_config {\n",
      "}\n",
      "ack_deadline_seconds: 10\n",
      "message_retention_duration {\n",
      "  seconds: 604800\n",
      "}\n",
      "expiration_policy {\n",
      "  ttl {\n",
      "    seconds: 2678400\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Creating PubSub Subscription:\n",
    "subscriber = pubsub_v1.SubscriberClient(credentials = cred)\n",
    "topic_path = subscriber.topic_path(project_id, topic_name)\n",
    "subscription_path = subscriber.subscription_path(project_id, subscription_name)\n",
    "subscription = subscriber.create_subscription(subscription_path, topic_path)\n",
    "print('Subscription created: {}'.format(subscription))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/subscription.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Query Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset gcp-bigdata-analytics-pipeline.AWS_Product_Review\n"
     ]
    }
   ],
   "source": [
    "#Creating a BigQuery Dataset:\n",
    "client = bigquery.Client(project=project_id, credentials=cred)\n",
    "dataset_id = client.project+\".\"+dataset_name\n",
    "dataset = bigquery.Dataset(dataset_id)\n",
    "dataset.location = \"US\"\n",
    "dataset = client.create_dataset(dataset)\n",
    "print(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Table schema from a JSON file:\n",
    "table_schema_data = json.loads(open(schema).read())\n",
    "table_schema = []\n",
    "for line in table_schema_data:\n",
    "    col = bigquery.SchemaField(name=line['name'], field_type=line['type'], mode=line['mode'], description=line['description'])\n",
    "    table_schema.append(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Big Query Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table gcp-bigdata-analytics-pipeline.AWS_Product_Review.Reviews\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Creating a BigQuery Table under the Datset created:\n",
    "table_id = project_id+\".\"+dataset_name+\".\"+table_name\n",
    "table = bigquery.Table(table_id, schema=table_schema)\n",
    "table = client.create_table(table)\n",
    "print(\"Created table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/big query table.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-Time streaming data\n",
    "\n",
    "We will now simulate the real-time data capture at big query database using GCP pub/sub and data flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getJobId(result):\n",
    "    r_list = [\"\\\\n\", \"\\\\r\", \"\\\"\", \" \", \"{\", \"}\"]\n",
    "    data = result\n",
    "    for item in r_list:\n",
    "        data = str(data).replace(item, \"\")\n",
    "    job_id = data.split(\",\")[2].split(\":\")[1]\n",
    "    return job_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job: 2019-12-23_16_31_24-8684177415575336791 is created successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Creating a Dataflow for PubSub to Big Query using template:\n",
    "command = \"gcloud dataflow jobs run {} --gcs-location gs://dataflow-templates/latest/PubSub_Subscription_to_BigQuery \".format(job_name)\n",
    "command += \"--parameters \"\n",
    "command += \"outputTableSpec={}:{}.{},\".format(project_id, dataset_name, table_name)\n",
    "command += \"inputSubscription=projects/{}/subscriptions/{} --format={}\".format(project_name, subscription_name, \"json\")\n",
    "\n",
    "result = subprocess.check_output(command, shell=True)\n",
    "job_id = getJobId(result)\n",
    "\n",
    "print(\"Job: {} is created successfully!\".format(job_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an event/message to publish\n",
    "\n",
    "message_data = {\n",
    "                \"marketplace\":\"GCP-Pipeline\",\n",
    "                \"customer_id\":18778586,\n",
    "                \"review_id\":\"RDIJS7QYB6XNR\",\n",
    "                \"product_id\":\"B00EDBY7X8\",\n",
    "                \"product_parent\":122952789,\n",
    "                \"product_title\":\"Sample Message\",\n",
    "                \"product_category\": \"Toys\",\n",
    "                \"star_rating\":5,\n",
    "                \"helpful_votes\":0,\n",
    "                \"total_votes\":0,\n",
    "                \"vine\": \"False\",\n",
    "                \"verified_purchase\": \"True\",\n",
    "                \"review_headline\": \"Five Stars\",\n",
    "                \"review_body\": \"Excellent!!!\",\n",
    "                \"review_date\": \"2015-08-31\"\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formatting message data before publishing:\n",
    "message_data = json.dumps(message_data)\n",
    "message_data = message_data.encode('utf-8')\n",
    "\n",
    "#Publishing a message on the PubSub Topic Created:\n",
    "response = publisher.publish(topic_path, message_data , origin='python-sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/running job.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/real-time-data.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Processing\n",
    "\n",
    "Consider we already have historical data for our application which we need to analyse\n",
    "For that we have to dump it to big query database\n",
    "\n",
    "We have the data located on Local Machine, we can then use Google cloud SDK bq function to load data directly to big query database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 number of processors\n",
      "Total Number of file to upload: 2\n"
     ]
    }
   ],
   "source": [
    "input_folder = r\"./Data/\"\n",
    "file_type = \".tsv.gz\"\n",
    "\n",
    "NUM_WORKERS = int(os.environ['NUMBER_OF_PROCESSORS'])\n",
    "print(\"There are {} number of processors\".format(NUM_WORKERS))\n",
    "\n",
    "files = glob.glob(input_folder+\"*.tsv.gz\")\n",
    "print(\"Total Number of file to upload: {}\".format(len(files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uploadFileToBigQueryTable(filename):\n",
    "    global project_id\n",
    "    path, file = os.path.split(filename)\n",
    "    print(\"Uploading file {} on big query \\n\".format(file))\n",
    "    command = \"bq --location=US load --null_marker=NULL --skip_leading_rows 1 --quote \\\"\\\" \"\n",
    "    command += \"-E UTF-8 --source_format=CSV --field_delimiter \\\\t {}.{} {}\".format(dataset_name, table_name, filename)\n",
    "    \n",
    "    setup_command = \"gcloud config set project {}\".format(project_id)\n",
    "    result = subprocess.check_output(setup_command, shell=True)\n",
    "    result = subprocess.check_output(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file amazon_reviews_us_Gift_Card_v1_00.tsv.gz on big query \n",
      "Uploading file amazon_reviews_us_Personal_Care_Appliances_v1_00.tsv.gz on big query \n",
      "\n",
      "\n",
      "Total Time for uploading of files is 41.91423273086548 secs!\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:\n",
    "    futures = {executor.submit(uploadFileToBigQueryTable, file) for file in files}\n",
    "    concurrent.futures.wait(futures)\n",
    "    \n",
    "executor.shutdown()\n",
    "end = time.time()\n",
    "print(\"Total Time for uploading of files is {} secs!\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/bulk-data.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analytics\n",
    "\n",
    "As our data is available in the Big Query table, we can connect to it through any analytics tool like GCP Data Explorer, Tableau, Power BI and so on.\n",
    "\n",
    "If the connection to big query is live then every time we capture any new data through our real-time data streaming data flow, we can see the related anallytics on the dashboard instantly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teardown the Infrastrucuture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted table 'gcp-bigdata-analytics-pipeline.AWS_Product_Review.Reviews'.\n"
     ]
    }
   ],
   "source": [
    "#Deleting table\n",
    "\n",
    "client = bigquery.Client(project=project_id, credentials=cred)\n",
    "client.delete_table(table_id, not_found_ok=True)\n",
    "print(\"Deleted table '{}'.\".format(table_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted dataset 'gcp-bigdata-analytics-pipeline.AWS_Product_Review'.\n"
     ]
    }
   ],
   "source": [
    "#Deleting Dataset\n",
    "\n",
    "client = bigquery.Client(project=project_id, credentials=cred)\n",
    "client.delete_dataset(\n",
    "    dataset_id, delete_contents=True, not_found_ok=True\n",
    ")  # Make an API request.\n",
    "\n",
    "print(\"Deleted dataset '{}'.\".format(dataset_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b''"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Deleting Running data flow\n",
    "\n",
    "command = \"gcloud dataflow jobs cancel {} --project {}\".format(job_id, project_id)\n",
    "subprocess.check_output(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b''\n"
     ]
    }
   ],
   "source": [
    "# Deleting data storage\n",
    "\n",
    "command = \"gsutil rm -r gs://{}\".format(bucket_name)\n",
    "response = subprocess.check_output(command, shell=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subscription deleted: projects/gcp-bigdata-analytics-pipeline/subscriptions/gcp-subscription\n"
     ]
    }
   ],
   "source": [
    "#Deleting PubSub Subscription\n",
    "\n",
    "subscriber = pubsub_v1.SubscriberClient(credentials = cred)\n",
    "topic_path = subscriber.topic_path(project_id, topic_name)\n",
    "subscription_path = subscriber.subscription_path(project_id, subscription_name)\n",
    "\n",
    "subscription = subscriber.delete_subscription(subscription_path)\n",
    "print('Subscription deleted: {}'.format(subscription_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic deleted: projects/gcp-bigdata-analytics-pipeline/topics/gcp-topic\n"
     ]
    }
   ],
   "source": [
    "# Deleting Pubsub topic\n",
    "\n",
    "publisher = pubsub_v1.PublisherClient(credentials=cred)\n",
    "topic_path = publisher.topic_path(project_id, topic_name)\n",
    "publisher.delete_topic(topic_path)\n",
    "print(\"Topic deleted: {}\".format(topic_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
